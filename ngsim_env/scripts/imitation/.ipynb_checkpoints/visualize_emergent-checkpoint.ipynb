{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VISUALIZE_EMERGENT\n",
    "This file is used to visual KL divergence of distributions of emergent characteristics as well as absolute emergent behvaior values. \n",
    "\n",
    "The first step is to load the expert trajectories and compute the baseline values.\n",
    "\n",
    "Then we load the trajectories for the models that we want to examine.\n",
    "\n",
    "This involves some reshaping of the trajectories, and unnormalizing. \n",
    "\n",
    "Finally, we can compute the values we want, and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import hgail.misc.utils\n",
    "\n",
    "import utils\n",
    "import visualize_utils\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(x, mean, std):\n",
    "    return (x * std) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename2label(fn):\n",
    "    s = fn.find('-') + 1\n",
    "    e = fn.rfind('.')\n",
    "    return fn[s:e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn: trajdata_i101_trajectories-0750am-0805am.txt t: 0750am-0805am\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2fb6fd007264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnormalize_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclip_std_multiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mngsim_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;32m/home/meng/mit/safe_irl/ref/pg2_src/ngsim_env/scripts/imitation/utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filepath, act_keys, ngsim_filename, debug_size, min_length, normalize_data, shuffle, act_low, act_high, clip_std_multiple, debug_args)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"critic expert data: shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "expert_filepath = '/home/meng/bakup_trajs/ngsim_all.h5'\n",
    "\n",
    "filenames = list(utils.NGSIM_FILENAME_TO_ID.keys())\n",
    "\n",
    "expert = dict()\n",
    "for fn in filenames:\n",
    "    t = filename2label(fn)\n",
    "    print(\"fn:\", fn, \"t:\", t)\n",
    "    expert[t] = utils.load_data(\n",
    "        expert_filepath, \n",
    "        min_length=250, \n",
    "        normalize_data=False, \n",
    "        clip_std_multiple=10.,\n",
    "        ngsim_filename=fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, names = utils.load_x_feature_names(expert_filepath, 'trajdata_i101_trajectories-0750am-0805am.txt')\n",
    "observation_indexes = dict([(names[i], i) for i in range(len(names))])\n",
    "#print(observation_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies expert_values and bounds in place\n",
    "def compute_expert_for_timeperiod(obser_attrs, key_for_attr, expert_values, expert_bounds, expert,\n",
    "                                  time_period='0750am-0805am'):\n",
    "    expert_values[time_period] = dict()\n",
    "    expert_bounds[time_period] = dict()\n",
    "    t = time_period # so i dont have to write it out every time\n",
    "    for attr in obser_attrs:\n",
    "        expert_values[time_period][key_for_attr[attr]] = gaussian_kde(expert[time_period]\\\n",
    "                                                                      ['observations'][:,observation_indexes[attr]])\n",
    "        #lower = np.percentile(expert[time_period]['observations'][:,observation_indexes[attr]], 1)\n",
    "        #upper = np.percentile(expert[time_period]['observations'][:,observation_indexes[attr]], 99)\n",
    "        lower = np.min(expert[time_period]['observations'][:,observation_indexes[attr]])\n",
    "        upper = np.max(expert[time_period]['observations'][:,observation_indexes[attr]])\n",
    "        expert_bounds[time_period][key_for_attr[attr]] = (lower, upper)\n",
    "\n",
    "    # Some specific formatting ones\n",
    "    expert_values[time_period]['iTTC'] = gaussian_kde(1/expert[time_period]\\\n",
    "                                                      ['observations'][:,observation_indexes['time_to_collision']])\n",
    "    #lower = np.percentile(1/expert[time_period]['observations'][:,observation_indexes['time_to_collision']], 1)\n",
    "    #upper = np.percentile(1/expert[time_period]['observations'][:,observation_indexes['time_to_collision']], 99)\n",
    "    lower = np.min(1/expert[time_period]['observations'][:,observation_indexes['time_to_collision']])\n",
    "    upper = np.max(1/expert[time_period]['observations'][:,observation_indexes['time_to_collision']])\n",
    "    expert_bounds[time_period]['iTTC'] = (lower, upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_attributes = ['accel','turn_rate_global',\n",
    "                          'velocity', 'jerk', 'time_to_collision', \n",
    "                          'distance_road_edge_right', 'distance_road_edge_left']\n",
    "key_for_attr = {'velocity': 'Speed',\n",
    "                'jerk': 'Jerk',\n",
    "                'time_to_collision': 'TTC',\n",
    "                'distance_road_edge_left': 'dleft',\n",
    "                'distance_road_edge_right': 'dright',\n",
    "                'accel': 'Acceleration',\n",
    "                'turn_rate_global': 'Turn-rate'}\n",
    "\n",
    "expert_values = dict()\n",
    "expert_bounds = dict()\n",
    "timeperiod_labels = [filename2label(fn) for fn in filenames]\n",
    "\n",
    "\n",
    "for time_period in timeperiod_labels:\n",
    "    compute_expert_for_timeperiod(observation_attributes, key_for_attr, expert_values, expert_bounds, \n",
    "                                  expert, time_period)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expert_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = [\n",
    "    'singleagent_def_1_fine',\n",
    "    'singleagent_def_2_fine',\n",
    "    'singleagent_def_3_fine',\n",
    "    'multiagent_curr_1_fine',\n",
    "    'multiagent_curr_2_fine',\n",
    "    'multiagent_curr_3_fine'\n",
    "]\n",
    "n_itrs = dict([(i, 200) for i in model_labels])\n",
    "for i in model_labels:\n",
    "    if 'multi' in i:\n",
    "        n_itrs[i] = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_traj_lab_dict = visualize_utils.get_trajs_dict(model_labels)\n",
    "valdirs, params_filepaths = visualize_utils.get_val_dirs_and_params_paths_d(model_labels, n_itrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traj_lab_dict Structure:\n",
    "\n",
    "* for each model, a tuple of (trajectories, labels) that are returned from utils.load_trajs_labels()\n",
    "    \n",
    "    * trajectories is a list \n",
    "    \n",
    "        * there is a list element for each dataset (e.g. i101_750_805, i101_805_815..., so 6 elements)\n",
    "        \n",
    "            * This is determined from the validate run. Each dataset that we generate trajectories for (filenames variable in the main function for validate.py)\n",
    "        \n",
    "        * each list element is an ndarry of shape (T,), where T = number of simulations run. \n",
    "                * This is determined in validate.py, as run_args.n_multiagent_trajs / args.n_envs. For n_trajs = 10000, n_vehs = 100, T = 100.\n",
    "        \n",
    "                * validate.py produces a total of run_args.n_multiagent_trajs trajectories. From each initial sample scene, we simulate 1 trajectory for each agent. Thus, n_vehs trajectories. Therefore, we need to run T simulations. \n",
    "         \n",
    "            * each item in the array is a dictionary, of trajectory 'characteristics'\n",
    "            \n",
    "               * a trajectory component is something like, observations, actions:\n",
    "           \n",
    "                   * dict_keys(['rewards', 'rmse_t', 'prev_action', 'x', 'observations', 'mean', 'is_colliding', 'actions', 'y', 'log_std', 'rmse_vel', 'rmse_pos', 'phi', 's'])\n",
    "                  \n",
    "                   * the values are an array of shape (H, N, K), where H = the env_H (length of a trajectory or number of timesteps). N = number of agents. K is a third dimension, for things like observations (for which K=64). For something like rmse_t, K doesn't exist (array of shape (H, N)).\n",
    "                   \n",
    "                   * There is essentially a dictionary for every intial scene that we sampled. \n",
    "    \n",
    "    - labels is a list\n",
    "    \n",
    "        * There is also an element for each dataset, like for trajectories. So, 6 in the default case we have.\n",
    "    \n",
    "            + ['0750am-0805am', '0805am-0820am', '0820am-0835am', '0400-0415', '0500-0515', '0515-0530']\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we unnormalize / reshape for the multiagent viz case\n",
    "# reshape basically flattens each attribute of the trajectory\n",
    "def reformat_trajectories(traj_lab_dict, length = 200, multi=True, reshape=True, \n",
    "                          unnormalize_data=True):\n",
    "    for i, model in enumerate(model_labels):\n",
    "        trajs = traj_lab_dict[model][0]\n",
    "        params = hgail.misc.utils.load_params(params_filepaths[i])\n",
    "        for timeperiod in trajs:\n",
    "            for traj in timeperiod:\n",
    "                if multi:\n",
    "                    n_veh = traj['observations'].shape[1]\n",
    "                else:\n",
    "                    n_veh = 1\n",
    "                if unnormalize_data:\n",
    "                    for j in range(n_veh):\n",
    "                        traj['observations'][:,j] = unnormalize(\n",
    "                            traj['observations'][:,j], \n",
    "                            params['normalzing']['obs_mean'],\n",
    "                            np.sqrt(params['normalzing']['obs_var'])\n",
    "                        )\n",
    "                if reshape:\n",
    "                    for attr in traj.keys():\n",
    "                        shape = traj[attr].shape\n",
    "                        if multi:\n",
    "                            if len(shape) > 0 and shape[0] == length and shape[1] == n_veh:\n",
    "                                traj[attr] = traj[attr].reshape(length*n_veh, -1)\n",
    "                            else:\n",
    "                                print(attr)\n",
    "                                print(traj[attr].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_lab_dict = orig_traj_lab_dict#deepcopy(orig_traj_lab_dict) #sometimes crashes kernel to deepcopy...\n",
    "reformat_trajectories(traj_lab_dict, reshape=True, unnormalize_data=True, length=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions that will be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(ref, comp, x, eps=1e-3):\n",
    "    return entropy(ref.pdf(x) + eps, comp.pdf(x) + eps)\n",
    "    logp = ref.logpdf(x)\n",
    "    p = np.exp(logp)\n",
    "    logq = comp.logpdf(x)\n",
    "    return - np.sum(p * (logq - logp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emergent_divergence(trajs, labels, bounds, expert, nbins=250):\n",
    "    # collect values\n",
    "    emergent = dict()\n",
    "    \n",
    "    for (timeperiod, l) in zip(trajs, labels):\n",
    "        emergent[l] = dict()\n",
    "        emergent[l]['Acceleration'] = np.concatenate([traj['observations'][:,observation_indexes['accel']] for traj in timeperiod])\n",
    "        emergent[l]['Turn-rate'] = np.concatenate([traj['observations'][:,observation_indexes['turn_rate_global']] for traj in timeperiod])\n",
    "        emergent[l]['Speed'] = np.concatenate([traj['observations'][:,observation_indexes['velocity']] for traj in timeperiod])\n",
    "        emergent[l]['Jerk'] = np.concatenate([traj['observations'][:,observation_indexes['jerk']] for traj in timeperiod])\n",
    "        #emergent[l]['TTC'] = np.concatenate([traj['observations'][:,observation_indexes['time_to_collision']] for traj in timeperiod])\n",
    "        emergent[l]['iTTC'] = np.concatenate([1/traj['observations'][:,observation_indexes['time_to_collision']] for traj in timeperiod])        \n",
    "        \n",
    "    # compute divergences\n",
    "    divergences = dict()\n",
    "    for l, l_values in emergent.items():\n",
    "        \n",
    "        if l not in expert.keys():\n",
    "            continue\n",
    "            \n",
    "        divergences[l] = dict()\n",
    "        for k, values in l_values.items():\n",
    "            print(k, l)\n",
    "            x = np.linspace(bounds[l][k][0], bounds[l][k][1], nbins)\n",
    "            divergences[l][k] = compute_kl(expert[l][k], gaussian_kde(values), x)\n",
    "            \n",
    "    return divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_divs(traj_lab_dict, model_labels, expert_bounds, expert_values):\n",
    "    model_divs = []\n",
    "    ngsim_labels = traj_lab_dict[model_labels[0]][1]\n",
    "    model_trajs = [traj_lab_dict[key][0] for key in model_labels]\n",
    "    for i, trajs in enumerate(model_trajs):\n",
    "        print('computing divergence for model {}'.format(model_labels[i]))\n",
    "        divs = compute_emergent_divergence(trajs, ngsim_labels, expert_bounds, expert_values)\n",
    "        model_divs.append(divs)\n",
    "    return model_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plots(model_divs, name='emergent'):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    attr_width = .2\n",
    "    for i, divs in enumerate(model_divs):\n",
    "\n",
    "        for (timeperiod, timeperiod_divs) in divs.items():\n",
    "            attr_keys = sorted(timeperiod_divs.keys())\n",
    "            for j, attr in enumerate(attr_keys):\n",
    "                plt.subplot(1,len(attr_keys),j+1)\n",
    "                plt.title(attr)\n",
    "                div = timeperiod_divs[attr]\n",
    "                plt.bar(i*attr_width, div, width=.15, label=model_labels[i])\n",
    "                if i == len(model_divs) - 1:\n",
    "                    plt.legend()\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below 2 cells after redefining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_divs = compute_divs(traj_lab_dict, model_labels, expert_bounds, expert_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_plots(model_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_plots_2(emergent_values, sorted_order, name='emergent2'):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    attr_width = .2\n",
    "    for i, model in enumerate(sorted_order):\n",
    "        divs = emergent_values[model]\n",
    "        attr_keys = sorted(emergent_values[model].keys())\n",
    "        for j, attr in enumerate(attr_keys):\n",
    "            plt.subplot(1,len(attr_keys),j+1)\n",
    "            plt.title(attr)\n",
    "            div = divs[attr]\n",
    "            plt.bar(i*attr_width, div, width=.15, label=model)\n",
    "            if i == len(emergent_values) - 1:\n",
    "                plt.legend()\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngsim_labels = traj_lab_dict[model_labels[0]][1]\n",
    "\n",
    "for t in ngsim_labels:\n",
    "    print(t)\n",
    "    model_divs_avg = dict()\n",
    "    model_divs_avg['average_single'] = dict()\n",
    "    model_divs_avg['average_multi'] = dict()\n",
    "    for attr in model_divs[0][t].keys():\n",
    "        model_divs_avg['average_single'][attr] = np.mean([model_divs[i][t][attr] \\\n",
    "                                                            for i, label in enumerate(model_labels) \\\n",
    "                                                              if 'single' in label])\n",
    "        model_divs_avg['average_multi'][attr] = np.mean([model_divs[i][t][attr]\\\n",
    "                                                            for i, label in enumerate(model_labels) \\\n",
    "                                                              if 'multi' in label])\n",
    "    avg_sorted_order = ['average_single', 'average_multi']\n",
    "    do_plots_2(model_divs_avg, avg_sorted_order, 'emergent_avg_'+t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngsim_labels = traj_lab_dict[model_labels[0]][1]\n",
    "\n",
    "to_average = ['0805am-0820am', '0820am-0835am']\n",
    "n = len(to_average)\n",
    "\n",
    "\n",
    "model_divs_avg = dict()\n",
    "model_divs_avg['average_single'] = dict()\n",
    "model_divs_avg['average_multi'] = dict()\n",
    "\n",
    "for t in to_average:\n",
    "    print(t)\n",
    "    for attr in model_divs[0][t].keys():\n",
    "        if attr in model_divs_avg['average_single'].keys():\n",
    "            model_divs_avg['average_single'][attr] += np.mean([model_divs[i][t][attr] \\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'single' in label])/n\n",
    "            model_divs_avg['average_multi'][attr] += np.mean([model_divs[i][t][attr]\\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'multi' in label])/n\n",
    "        else:\n",
    "            model_divs_avg['average_single'][attr] = np.mean([model_divs[i][t][attr] \\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'single' in label])/n\n",
    "            model_divs_avg['average_multi'][attr] = np.mean([model_divs[i][t][attr]\\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'multi' in label])/n\n",
    "avg_sorted_order = ['average_single', 'average_multi']\n",
    "print(model_divs_avg)\n",
    "do_plots_2(model_divs_avg, avg_sorted_order, 'emergent_avg_'+'_'.join(to_average))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngsim_labels = traj_lab_dict[model_labels[0]][1]\n",
    "\n",
    "to_average = ['0400-0415', '0500-0515', '0515-0530']\n",
    "n = len(to_average)\n",
    "\n",
    "model_divs_avg = dict()\n",
    "model_divs_avg['average_single'] = dict()\n",
    "model_divs_avg['average_multi'] = dict()\n",
    "\n",
    "for t in to_average:\n",
    "    print(t)\n",
    "    for attr in model_divs[0][t].keys():\n",
    "        if attr in model_divs_avg['average_single'].keys():\n",
    "            model_divs_avg['average_single'][attr] += np.mean([model_divs[i][t][attr] \\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'single' in label])/n\n",
    "            model_divs_avg['average_multi'][attr] += np.mean([model_divs[i][t][attr]\\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'multi' in label])/n\n",
    "        else:\n",
    "            model_divs_avg['average_single'][attr] = np.mean([model_divs[i][t][attr] \\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'single' in label])/n\n",
    "            model_divs_avg['average_multi'][attr] = np.mean([model_divs[i][t][attr]\\\n",
    "                                                                for i, label in enumerate(model_labels) \\\n",
    "                                                                  if 'multi' in label])/n\n",
    "avg_sorted_order = ['average_single', 'average_multi']\n",
    "do_plots_2(model_divs_avg, avg_sorted_order, 'emergent_avg_'+'_'.join(to_average))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# We also want to compute actual emergent values\n",
    "These are: Lane change rate, Offroad duration, Collision Rate, and Hard Brake Rate\n",
    "\n",
    "The above graph is for KL divergence of features, which is not the same as the emergent characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_offroad(traj, thresh = -1.0):\n",
    "    num_offroad = float(len(traj['observations'][:,0][\n",
    "        np.where(np.minimum(traj['observations'][:,observation_indexes['distance_road_edge_left']], \n",
    "                            traj['observations'][:,observation_indexes['distance_road_edge_right']]) <= thresh)\n",
    "        ]))\n",
    "    return num_offroad / float(len(traj['observations']))\n",
    "\n",
    "def calc_hard_brake(traj, thresh = -3.0):\n",
    "    #traj should be unnormalized\n",
    "    return float(len(traj['observations'][:,observation_indexes['accel']][np.where(\\\n",
    "                traj['observations'][:,observation_indexes['accel']] <= thresh)])) \\\n",
    "                / float(len(traj['actions']))\n",
    "\n",
    "def calc_collisions(traj, expert=True, H=200, nveh=100):\n",
    "    if expert:\n",
    "        return float(len(traj['observations'][:,observation_indexes['is_colliding']][\n",
    "            np.where(np.isclose(traj['observations'][:,observation_indexes['is_colliding']],1))\n",
    "        ]))/ float(len(traj['observations']))\n",
    "    else:\n",
    "        a = np.reshape(traj['observations'][:,observation_indexes['is_colliding']], (H, nveh))[:50,:]\n",
    "        return np.mean(np.any(np.isclose(a, 1.0), axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_lane_change(single_traj_d_right, lane_width=3.0, max_jump=1.5, verbose=False):\n",
    "    num = 0.0\n",
    "    i = 0\n",
    "    d0 = single_traj_d_right[0]\n",
    "    while i < len(single_traj_d_right):\n",
    "        indxs = np.argwhere(abs(single_traj_d_right[i:] - d0) >= lane_width)\n",
    "        if verbose:\n",
    "            print(\"indxs\", indxs)\n",
    "            print(i)\n",
    "            print(\"old d0\", d0)\n",
    "        if len(indxs) == 0: \n",
    "            return num\n",
    "        i += indxs[0][0]\n",
    "        d0 = single_traj_d_right[i]\n",
    "        if verbose:\n",
    "            print(i)\n",
    "            print(d0)\n",
    "        if i > 0 and abs(d0 - single_traj_d_right[i-1]) <= max_jump:\n",
    "            if verbose: print(\"increase\")\n",
    "            num += 1\n",
    "    return num\n",
    "\n",
    "def test_count_num_lane_change():\n",
    "    single_traj = np.array([0,1.5])\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1, verbose=False) == 0\n",
    "    single_traj = np.array([0,1,1.5])\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1, verbose=False) == 1\n",
    "\n",
    "    single_traj = np.array([0,0,0.5,1.1,1,1,1.5,\\\n",
    "                   2.0,2.6,2.4,3.0,\\\n",
    "                  1,2,1.5,2,2.5])\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1) == 3\n",
    "    single_traj[-1] = 2.4\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1) == 2\n",
    "    single_traj[-1] = 2.6\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1) == 3\n",
    "\n",
    "    single_traj = np.array([0,0,5,5,5,5.5,6.5,\\\n",
    "                   2.5,2.6,2.4,3.0,3.6,4.0,\\\n",
    "                  1,2,1.5,2,2.5])\n",
    "    assert count_num_lane_change(single_traj, lane_width=1.5, max_jump=1, verbose=False) == 3\n",
    "    \n",
    "test_count_num_lane_change()\n",
    "\n",
    "def calc_lane_change(traj):\n",
    "    num_lane_change = count_num_lane_change(traj['observations'][:,observation_indexes['distance_road_edge_left']])\n",
    "    return num_lane_change / float(len(traj['observations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other specific metrics we want\n",
    "HARD_BRAKE_THRESH = -3.0\n",
    "expert_emergent_values = dict()\n",
    "for time_period in expert.keys():\n",
    "    print(time_period)\n",
    "    expert_emergent_values[time_period] = dict()\n",
    "    expert_emergent_values[time_period]['Collision Rate'] = calc_collisions(expert[time_period])\n",
    "    expert_emergent_values[time_period]['Offroad Duration'] = calc_offroad(expert[time_period])\n",
    "    expert_emergent_values[time_period]['Hard Brake Rate'] = calc_hard_brake(expert[time_period],\n",
    "                                                                                 thresh = HARD_BRAKE_THRESH)\n",
    "    #expert_emergent_values[time_period]['Lane Change Rate'] = calc_lane_change(expert[time_period])\n",
    "    #print(expert_emergent_values[time_period]['Lane Change Rate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below 3 cells after redefining the models\n",
    "\n",
    "### Small disclaimer that lane change rate is calculated very slowly... \n",
    "you may want to run without to start, it isn't super useful. Then start it up and go watch a movie, and maybe it'll be done when you come back :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngsim_labels = traj_lab_dict[model_labels[0]][1]\n",
    "N_VEH = 100\n",
    "sorted_order = ['expert']\n",
    "sorted_order.extend([i for i in sorted(model_labels) if 'single' in i])\n",
    "sorted_order.extend([i for i in sorted(model_labels) if 'multi' in i])\n",
    "print(sorted_order)\n",
    "\n",
    "emergent_values = dict()\n",
    "\n",
    "for t, t_label in enumerate(ngsim_labels):\n",
    "    print(t_label)\n",
    "    emergent_values[t_label] = dict()\n",
    "    emergent_values[t_label]['expert'] = expert_emergent_values[t_label]\n",
    "    for label in model_labels:\n",
    "        print(label)\n",
    "        trajs = traj_lab_dict[label][0][t]\n",
    "        emergent_values[t_label][label] = dict()\n",
    "        emergent_values[t_label][label]['Hard Brake Rate'] = np.mean([calc_hard_brake(scene, thresh = HARD_BRAKE_THRESH\n",
    "                                                                           ) for scene in trajs])\n",
    "        emergent_values[t_label][label]['Offroad Duration'] = np.mean([calc_offroad(scene) for scene in trajs])\n",
    "        emergent_values[t_label][label]['Collision Rate'] = np.mean([calc_collisions(scene, expert=True, nveh=N_VEH) for scene in trajs])\n",
    "        #emergent_values[t_label][label]['Lane Change Rate'] = np.mean([calc_lane_change(scene) for scene in trajs])\n",
    "\n",
    "    #do_plots_2(emergent_values[t_label], sorted_order, 'emergent_2_'+t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ngsim_labels:\n",
    "    do_plots_2(emergent_values[t], sorted_order, 'emergent_2_'+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sorted_order = ['expert', 'average_single', 'average_multi']\n",
    "average_emergent_values = dict()\n",
    "for t, t_label in enumerate(ngsim_labels):\n",
    "    print(t_label)\n",
    "    average_emergent_values[t_label] = dict()\n",
    "    average_emergent_values[t_label]['expert'] = expert_emergent_values[t_label]\n",
    "    average_emergent_values[t_label]['average_single'] = dict()\n",
    "    average_emergent_values[t_label]['average_multi'] = dict()\n",
    "\n",
    "    for attr in average_emergent_values[t_label]['expert'].keys():\n",
    "        average_emergent_values[t_label]['average_single'][attr] = np.mean([emergent_values[t_label][label][attr] \\\n",
    "                                                                   for label in model_labels if 'single' in label])\n",
    "        average_emergent_values[t_label]['average_multi'][attr] = np.mean([emergent_values[t_label][label][attr]\\\n",
    "                                                                 for label in model_labels if 'multi' in label])\n",
    "    do_plots_2(average_emergent_values[t_label], avg_sorted_order, 'emergent_avg2_'+t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_average = ['0400-0415', '0500-0515', '0515-0530']\n",
    "n = len(to_average)\n",
    "\n",
    "label = \"_\".join(to_average)\n",
    "val_average_emergent_values = dict()\n",
    "val_average_emergent_values[label] = dict()\n",
    "\n",
    "for model in avg_sorted_order:\n",
    "    val_average_emergent_values[label][model] = dict()\n",
    "    for attr in average_emergent_values[to_average[0]][model].keys():\n",
    "        val_average_emergent_values[label][model][attr] = np.mean([average_emergent_values[t][model][attr] \\\n",
    "                                                               for t in to_average])\n",
    "\n",
    "do_plots_2(val_average_emergent_values[label], avg_sorted_order, 'emergent_avg2_'+label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_average = ['0805am-0820am', '0820am-0835am']\n",
    "\n",
    "n = len(to_average)\n",
    "\n",
    "label = \"_\".join(to_average)\n",
    "\n",
    "val_average_emergent_values[label] = dict()\n",
    "\n",
    "for model in avg_sorted_order:\n",
    "    val_average_emergent_values[label][model] = dict()\n",
    "    for attr in average_emergent_values[to_average[0]][model].keys():\n",
    "        val_average_emergent_values[label][model][attr] = np.mean([average_emergent_values[t][model][attr] \\\n",
    "                                                               for t in to_average])\n",
    "\n",
    "print(val_average_emergent_values[label])\n",
    "do_plots_2(val_average_emergent_values[label], avg_sorted_order, 'emergent_avg2_'+label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
